<document xmlns="http://cnx.rice.edu/cnxml" xmlns:m="http://www.w3.org/1998/Math/MathML" xmlns:md="http://cnx.rice.edu/mdml" xmlns:bib="http://bibtexml.sf.net/">
  <title>Nonparametric regression with wavelets</title>
  <metadata>
  <md:content-id>m17396</md:content-id><md:title>Nonparametric regression with wavelets</md:title>
  <md:abstract/>
  <md:uuid>8f555a88-0c87-429d-8887-4624cf0b2ba9</md:uuid>
</metadata>

<content>
    
    
      <para id="id2253740">In this section, we consider only real-valued wavelet functions that form an orthogonal basis, hence <m:math><m:mrow><m:mi>ϕ</m:mi><m:mo>≡</m:mo><m:mover accent="true"><m:mi>ϕ</m:mi><m:mo>˜</m:mo></m:mover></m:mrow></m:math>
and <m:math><m:mrow><m:mi>ψ</m:mi><m:mo>≡</m:mo><m:mover accent="true"><m:mi>ψ</m:mi><m:mo>˜</m:mo></m:mover></m:mrow></m:math>.
We saw in <link document="m17394" target-id="uid22">Orthogonal Bases from Multiresolution analysis and wavelets</link><!-- section 3.3--> how a given function belonging to <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mrow><m:mo>(</m:mo><m:mi mathvariant="double-struck">R</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> could be represented as
a wavelet series. Here, we explain how to use a wavelet basis to construct a nonparametric estimator for the regression function <m:math><m:mi>m</m:mi></m:math> in the model</para>
      <equation id="uid1">
        <m:math mode="display">
          <m:mrow>
            <m:msub>
              <m:mi>Y</m:mi>
              <m:mi>i</m:mi>
            </m:msub>
            <m:mo>=</m:mo>
            <m:mi>m</m:mi>
            <m:mrow>
              <m:mo>(</m:mo>
              <m:msub>
                <m:mi>x</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mo>)</m:mo>
            </m:mrow>
            <m:mo>+</m:mo>
            <m:msub>
              <m:mi>ϵ</m:mi>
              <m:mi>i</m:mi>
            </m:msub>
            <m:mo>,</m:mo>
            <m:mspace width="0.277778em"/>
            <m:mi>i</m:mi>
            <m:mo>=</m:mo>
            <m:mn>1</m:mn>
            <m:mo>,</m:mo>
            <m:mo>...</m:mo>
            <m:mo>,</m:mo>
            <m:mi>n</m:mi>
            <m:mo>,</m:mo>
            <m:mspace width="0.277778em"/>
            <m:mi>n</m:mi>
            <m:mo>=</m:mo>
            <m:msup>
              <m:mn>2</m:mn>
              <m:mi>J</m:mi>
            </m:msup>
            <m:mo>,</m:mo>
            <m:mspace width="0.277778em"/>
            <m:mi>J</m:mi>
            <m:mo>∈</m:mo>
            <m:mi mathvariant="double-struck">N</m:mi>
            <m:mspace width="3.33333pt"/>
            <m:mo>,</m:mo>
          </m:mrow>
        </m:math>
      </equation>
      <para id="id2254174">where <m:math><m:mrow><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>=</m:mo><m:mfrac><m:mi>i</m:mi><m:mi>n</m:mi></m:mfrac></m:mrow></m:math> are equispaced design points and the errors are i.i.d. Gaussian, <m:math><m:mrow><m:msub><m:mi>ϵ</m:mi><m:mi>i</m:mi></m:msub><m:mspace width="3.33333pt"/><m:mo>∼</m:mo><m:mspace width="3.33333pt"/><m:mi>N</m:mi><m:mrow><m:mo>(</m:mo><m:mn>0</m:mn><m:mo>,</m:mo><m:msubsup><m:mi>σ</m:mi><m:mi>ϵ</m:mi><m:mn>2</m:mn></m:msubsup><m:mo>)</m:mo></m:mrow></m:mrow></m:math>.</para>
      <para id="id2254249">A wavelet estimator can be <emphasis>linear</emphasis> or <emphasis>nonlinear</emphasis>.
The linear wavelet estimator proceeds by projecting the data onto a coarse level space. This estimator is of a kernel-type, see <link target-id="uid2">"Linear smoothing with wavelets"</link>.
Another possibility for estimating <m:math><m:mi>m</m:mi></m:math> is to detect which detail coefficients convey
the important information about the function <m:math><m:mi>m</m:mi></m:math> and to put equal
to zero all the other coefficients. This yields a nonlinear wavelet estimator as described in <link target-id="uid14">"Nonlinear smoothing with wavelets"</link>.</para>
      
      <section id="uid2">
        <title>Linear smoothing with wavelets</title>
        <para id="id2254321">Suppose we are given data <m:math><m:msubsup><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>,</m:mo><m:msub><m:mi>Y</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>n</m:mi></m:msubsup></m:math> coming from the model <link target-id="uid1"/> and an orthogonal wavelet basis generated by <m:math><m:mrow><m:mo>{</m:mo><m:mi>ϕ</m:mi><m:mo>,</m:mo><m:mi>ψ</m:mi><m:mo>}</m:mo></m:mrow></m:math>.
The linear wavelet estimator  proceeds by choosing a cutting  level <m:math><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:math> and
represents an estimation of the projection of <m:math><m:mi>m</m:mi></m:math> onto the space <m:math><m:msub><m:mi>V</m:mi><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:msub></m:math>:</para>
        <equation id="uid5">
          <m:math mode="display">
            <m:mrow>
              <m:mover accent="true">
                <m:mi>m</m:mi>
                <m:mo>^</m:mo>
              </m:mover>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munderover>
                <m:mo>∑</m:mo>
                <m:mrow>
                  <m:mi>k</m:mi>
                  <m:mo>=</m:mo>
                  <m:mn>0</m:mn>
                </m:mrow>
                <m:mrow>
                  <m:msup>
                    <m:mn>2</m:mn>
                    <m:msub>
                      <m:mi>j</m:mi>
                      <m:mn>0</m:mn>
                    </m:msub>
                  </m:msup>
                  <m:mo>-</m:mo>
                  <m:mn>1</m:mn>
                </m:mrow>
              </m:munderover>
              <m:msub>
                <m:mover accent="true">
                  <m:mi>s</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>0</m:mn>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:msub>
                <m:mi>ϕ</m:mi>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>0</m:mn>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>+</m:mo>
              <m:munderover>
                <m:mo>∑</m:mo>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mo>=</m:mo>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>0</m:mn>
                  </m:msub>
                </m:mrow>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>1</m:mn>
                  </m:msub>
                  <m:mo>-</m:mo>
                  <m:mn>1</m:mn>
                </m:mrow>
              </m:munderover>
              <m:munderover>
                <m:mo>∑</m:mo>
                <m:mrow>
                  <m:mi>k</m:mi>
                  <m:mo>=</m:mo>
                  <m:mn>0</m:mn>
                </m:mrow>
                <m:mrow>
                  <m:msup>
                    <m:mn>2</m:mn>
                    <m:mi>j</m:mi>
                  </m:msup>
                  <m:mo>-</m:mo>
                  <m:mn>1</m:mn>
                </m:mrow>
              </m:munderover>
              <m:msub>
                <m:mover accent="true">
                  <m:mi>d</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:msub>
                <m:mi>ψ</m:mi>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munder>
                <m:mo>∑</m:mo>
                <m:mi>k</m:mi>
              </m:munder>
              <m:msub>
                <m:mover accent="true">
                  <m:mi>s</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>1</m:mn>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:msub>
                <m:mi>ϕ</m:mi>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>1</m:mn>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>,</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2254721">with <m:math><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub></m:math> the coarsest level in the decomposition, and
where the so-called <emphasis>empirical coefficients</emphasis> are computed as</para>
        <equation id="uid6">
          <m:math mode="display">
            <m:mrow>
              <m:msub>
                <m:mover accent="true">
                  <m:mi>s</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mo>=</m:mo>
              <m:mfrac>
                <m:mn>1</m:mn>
                <m:mi>n</m:mi>
              </m:mfrac>
              <m:munderover>
                <m:mo>∑</m:mo>
                <m:mrow>
                  <m:mi>i</m:mi>
                  <m:mo>=</m:mo>
                  <m:mn>1</m:mn>
                </m:mrow>
                <m:mi>n</m:mi>
              </m:munderover>
              <m:msub>
                <m:mi>Y</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mspace width="4pt"/>
              <m:msub>
                <m:mi>ϕ</m:mi>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:msub>
                  <m:mi>x</m:mi>
                  <m:mi>i</m:mi>
                </m:msub>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mspace width="1.em"/>
              <m:mtext>and</m:mtext>
              <m:mspace width="1.em"/>
              <m:msub>
                <m:mover accent="true">
                  <m:mi>d</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mo>=</m:mo>
              <m:mfrac>
                <m:mn>1</m:mn>
                <m:mi>n</m:mi>
              </m:mfrac>
              <m:munderover>
                <m:mo>∑</m:mo>
                <m:mrow>
                  <m:mi>i</m:mi>
                  <m:mo>=</m:mo>
                  <m:mn>1</m:mn>
                </m:mrow>
                <m:mi>n</m:mi>
              </m:munderover>
              <m:msub>
                <m:mi>Y</m:mi>
                <m:mi>i</m:mi>
              </m:msub>
              <m:mspace width="4pt"/>
              <m:msub>
                <m:mi>ψ</m:mi>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:msub>
                  <m:mi>x</m:mi>
                  <m:mi>i</m:mi>
                </m:msub>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mspace width="3.33333pt"/>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2254927">The cutting level <m:math><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:math> plays the role of a smoothing parameter: a small value of <m:math><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:math> means that many detail coefficients are left out, and this may lead to oversmoothing. On the other hand, if <m:math><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:math> is too large, too many coefficients will be kept, and some artificial bumps will probably remain in the estimation of <m:math><m:mrow><m:mi>m</m:mi><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow></m:math>.</para>
        <para id="id2254996">To see that the estimator <link target-id="uid5"/> is of a kernel-type, consider first the projection of <m:math><m:mi>m</m:mi></m:math> onto <m:math><m:msub><m:mi>V</m:mi><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:msub></m:math>: </para>
        <equation id="uid8">
          <m:math mode="display">
            <m:mtable displaystyle="true">
              <m:mtr>
                <m:mtd columnalign="right">
                  <m:mrow>
                    <m:msub>
                      <m:mi mathvariant="script">P</m:mi>
                      <m:msub>
                        <m:mi>V</m:mi>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>1</m:mn>
                        </m:msub>
                      </m:msub>
                    </m:msub>
                    <m:mi>m</m:mi>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                  </m:mrow>
                </m:mtd>
                <m:mtd>
                  <m:mo>=</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:munder>
                      <m:mo>∑</m:mo>
                      <m:mi>k</m:mi>
                    </m:munder>
                    <m:mfenced separators="" open="(" close=")">
                      <m:mrow>
                        <m:mo>∫</m:mo>
                        <m:mspace width="-0.166667em"/>
                        <m:mi>m</m:mi>
                        <m:mrow>
                          <m:mo>(</m:mo>
                          <m:mi>y</m:mi>
                          <m:mo>)</m:mo>
                        </m:mrow>
                        <m:mrow>
                          <m:msub>
                            <m:mi>ϕ</m:mi>
                            <m:mrow>
                              <m:msub>
                                <m:mi>j</m:mi>
                                <m:mn>1</m:mn>
                              </m:msub>
                              <m:mo>,</m:mo>
                              <m:mi>k</m:mi>
                            </m:mrow>
                          </m:msub>
                          <m:mrow>
                            <m:mo>(</m:mo>
                            <m:mi>y</m:mi>
                            <m:mo>)</m:mo>
                          </m:mrow>
                        </m:mrow>
                        <m:mi>d</m:mi>
                        <m:mi>y</m:mi>
                      </m:mrow>
                    </m:mfenced>
                    <m:msub>
                      <m:mi>ϕ</m:mi>
                      <m:mrow>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>1</m:mn>
                        </m:msub>
                        <m:mo>,</m:mo>
                        <m:mi>k</m:mi>
                      </m:mrow>
                    </m:msub>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
              <m:mtr>
                <m:mtd/>
                <m:mtd>
                  <m:mo>=</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:mo>∫</m:mo>
                    <m:mspace width="-0.166667em"/>
                    <m:msub>
                      <m:mi>K</m:mi>
                      <m:msub>
                        <m:mi>j</m:mi>
                        <m:mn>1</m:mn>
                      </m:msub>
                    </m:msub>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>,</m:mo>
                      <m:mi>y</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                    <m:mi>m</m:mi>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>y</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                    <m:mi>d</m:mi>
                    <m:mi>y</m:mi>
                    <m:mspace width="3.33333pt"/>
                    <m:mo>,</m:mo>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
            </m:mtable>
          </m:math>
        </equation>
        <para id="id2255238">where the (convolution) kernel <m:math><m:mrow><m:msub><m:mi>K</m:mi><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:msub><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>,</m:mo><m:mi>y</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> is given by</para>
        <equation id="id2255276">
          <m:math mode="display">
            <m:mrow>
              <m:msub>
                <m:mi>K</m:mi>
                <m:msub>
                  <m:mi>j</m:mi>
                  <m:mn>1</m:mn>
                </m:msub>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>,</m:mo>
                <m:mi>y</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munder>
                <m:mo>∑</m:mo>
                <m:mi>k</m:mi>
              </m:munder>
              <m:mrow>
                <m:msub>
                  <m:mi>ϕ</m:mi>
                  <m:mrow>
                    <m:msub>
                      <m:mi>j</m:mi>
                      <m:mn>1</m:mn>
                    </m:msub>
                    <m:mo>,</m:mo>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>y</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
              </m:mrow>
              <m:msub>
                <m:mi>ϕ</m:mi>
                <m:mrow>
                  <m:msub>
                    <m:mi>j</m:mi>
                    <m:mn>1</m:mn>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>k</m:mi>
                </m:mrow>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>x</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mspace width="3.33333pt"/>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2255377">Härdle <emphasis>et al.</emphasis> <link target-id="bid0"/> studied the approximation properties of this projection operator. In order to estimate <link target-id="uid8"/>, Antoniadis <emphasis>et al.</emphasis> <link target-id="bid1"/> proposed to take:</para>
        <equation id="uid9">
          <m:math mode="display">
            <m:mtable displaystyle="true">
              <m:mtr>
                <m:mtd columnalign="right">
                  <m:mrow>
                    <m:mover accent="true">
                      <m:msub>
                        <m:mi mathvariant="script">P</m:mi>
                        <m:msub>
                          <m:mi>V</m:mi>
                          <m:msub>
                            <m:mi>j</m:mi>
                            <m:mn>1</m:mn>
                          </m:msub>
                        </m:msub>
                      </m:msub>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mi>m</m:mi>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                  </m:mrow>
                </m:mtd>
                <m:mtd>
                  <m:mo>=</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:munderover>
                      <m:mo>∑</m:mo>
                      <m:mrow>
                        <m:mi>i</m:mi>
                        <m:mo>=</m:mo>
                        <m:mn>1</m:mn>
                      </m:mrow>
                      <m:mi>n</m:mi>
                    </m:munderover>
                    <m:msub>
                      <m:mi>Y</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:msubsup>
                      <m:mo>∫</m:mo>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:mi>i</m:mi>
                        <m:mo>-</m:mo>
                        <m:mn>1</m:mn>
                        <m:mo>)</m:mo>
                        <m:mo>/</m:mo>
                        <m:mi>n</m:mi>
                      </m:mrow>
                      <m:mrow>
                        <m:mi>i</m:mi>
                        <m:mo>/</m:mo>
                        <m:mi>n</m:mi>
                      </m:mrow>
                    </m:msubsup>
                    <m:msub>
                      <m:mi>K</m:mi>
                      <m:msub>
                        <m:mi>j</m:mi>
                        <m:mn>1</m:mn>
                      </m:msub>
                    </m:msub>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>,</m:mo>
                      <m:mi>y</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                    <m:mi>d</m:mi>
                    <m:mi>y</m:mi>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
              <m:mtr>
                <m:mtd/>
                <m:mtd>
                  <m:mo>=</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:munder>
                      <m:mo>∑</m:mo>
                      <m:mi>k</m:mi>
                    </m:munder>
                    <m:munderover>
                      <m:mo>∑</m:mo>
                      <m:mrow>
                        <m:mi>i</m:mi>
                        <m:mo>=</m:mo>
                        <m:mn>1</m:mn>
                      </m:mrow>
                      <m:mi>n</m:mi>
                    </m:munderover>
                    <m:msub>
                      <m:mi>Y</m:mi>
                      <m:mi>i</m:mi>
                    </m:msub>
                    <m:mfenced separators="" open="(" close=")">
                      <m:mrow>
                        <m:msubsup>
                          <m:mo>∫</m:mo>
                          <m:mrow>
                            <m:mo>(</m:mo>
                            <m:mi>i</m:mi>
                            <m:mo>-</m:mo>
                            <m:mn>1</m:mn>
                            <m:mo>)</m:mo>
                            <m:mo>/</m:mo>
                            <m:mi>n</m:mi>
                          </m:mrow>
                          <m:mrow>
                            <m:mi>i</m:mi>
                            <m:mo>/</m:mo>
                            <m:mi>n</m:mi>
                          </m:mrow>
                        </m:msubsup>
                        <m:mrow>
                          <m:msub>
                            <m:mi>ϕ</m:mi>
                            <m:mrow>
                              <m:msub>
                                <m:mi>j</m:mi>
                                <m:mn>1</m:mn>
                              </m:msub>
                              <m:mo>,</m:mo>
                              <m:mi>k</m:mi>
                            </m:mrow>
                          </m:msub>
                          <m:mrow>
                            <m:mo>(</m:mo>
                            <m:mi>y</m:mi>
                            <m:mo>)</m:mo>
                          </m:mrow>
                        </m:mrow>
                        <m:mi>d</m:mi>
                        <m:mi>y</m:mi>
                      </m:mrow>
                    </m:mfenced>
                    <m:msub>
                      <m:mi>ϕ</m:mi>
                      <m:mrow>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>1</m:mn>
                        </m:msub>
                        <m:mo>,</m:mo>
                        <m:mi>k</m:mi>
                      </m:mrow>
                    </m:msub>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                    <m:mspace width="3.33333pt"/>
                    <m:mo>.</m:mo>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
            </m:mtable>
          </m:math>
        </equation>
        <para id="id2255698">Approximating the last integral by <m:math><m:mrow><m:mfrac><m:mn>1</m:mn><m:mi>n</m:mi></m:mfrac><m:msub><m:mi>ϕ</m:mi><m:mrow><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub><m:mo>,</m:mo><m:mi>k</m:mi></m:mrow></m:msub><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:math>, we find back the estimator <m:math><m:mrow><m:mover accent="true"><m:mi>m</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> in <link target-id="uid5"/>.</para>
        <para id="id2255777">By orthogonality of the wavelet transform and Parseval's equality, the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk (or integrated mean square error IMSE) of a linear wavelet estimator is equal to the <m:math><m:mrow><m:msub><m:mi>l</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk of its wavelet coefficients:</para>
        <equation id="uid10">
          <m:math mode="display">
            <m:mtable displaystyle="true">
              <m:mtr>
                <m:mtd columnalign="right">
                  <m:mrow>
                    <m:mtext>IMSE</m:mtext>
                    <m:mo>=</m:mo>
                    <m:mi>E</m:mi>
                    <m:msubsup>
                      <m:mfenced separators="" open="∥" close="∥">
                        <m:mover accent="true">
                          <m:mi>m</m:mi>
                          <m:mo>^</m:mo>
                        </m:mover>
                        <m:mo>-</m:mo>
                        <m:mi>m</m:mi>
                      </m:mfenced>
                      <m:msub>
                        <m:mi>L</m:mi>
                        <m:mn>2</m:mn>
                      </m:msub>
                      <m:mn>2</m:mn>
                    </m:msubsup>
                  </m:mrow>
                </m:mtd>
                <m:mtd>
                  <m:mo>=</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:munder>
                      <m:mo>∑</m:mo>
                      <m:mi>k</m:mi>
                    </m:munder>
                    <m:mi>E</m:mi>
                    <m:msup>
                      <m:mrow>
                        <m:mo>[</m:mo>
                        <m:msub>
                          <m:mover accent="true">
                            <m:mi>s</m:mi>
                            <m:mo>^</m:mo>
                          </m:mover>
                          <m:mrow>
                            <m:msub>
                              <m:mi>j</m:mi>
                              <m:mn>0</m:mn>
                            </m:msub>
                            <m:mo>,</m:mo>
                            <m:mi>k</m:mi>
                          </m:mrow>
                        </m:msub>
                        <m:mo>-</m:mo>
                        <m:msubsup>
                          <m:mi>s</m:mi>
                          <m:mrow>
                            <m:msub>
                              <m:mi>j</m:mi>
                              <m:mn>0</m:mn>
                            </m:msub>
                            <m:mo>,</m:mo>
                            <m:mi>k</m:mi>
                          </m:mrow>
                          <m:mo>∘</m:mo>
                        </m:msubsup>
                        <m:mo>]</m:mo>
                      </m:mrow>
                      <m:mn>2</m:mn>
                    </m:msup>
                    <m:mo>+</m:mo>
                    <m:munderover>
                      <m:mo>∑</m:mo>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mo>=</m:mo>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>0</m:mn>
                        </m:msub>
                      </m:mrow>
                      <m:mrow>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>1</m:mn>
                        </m:msub>
                        <m:mo>-</m:mo>
                        <m:mn>1</m:mn>
                      </m:mrow>
                    </m:munderover>
                    <m:munder>
                      <m:mo>∑</m:mo>
                      <m:mi>k</m:mi>
                    </m:munder>
                    <m:mi>E</m:mi>
                    <m:msup>
                      <m:mrow>
                        <m:mo>[</m:mo>
                        <m:msub>
                          <m:mover accent="true">
                            <m:mi>d</m:mi>
                            <m:mo>^</m:mo>
                          </m:mover>
                          <m:mrow>
                            <m:mi>j</m:mi>
                            <m:mi>k</m:mi>
                          </m:mrow>
                        </m:msub>
                        <m:mo>-</m:mo>
                        <m:msubsup>
                          <m:mi>d</m:mi>
                          <m:mrow>
                            <m:mi>j</m:mi>
                            <m:mi>k</m:mi>
                          </m:mrow>
                          <m:mo>∘</m:mo>
                        </m:msubsup>
                        <m:mo>]</m:mo>
                      </m:mrow>
                      <m:mn>2</m:mn>
                    </m:msup>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
              <m:mtr>
                <m:mtd/>
                <m:mtd>
                  <m:mo>+</m:mo>
                </m:mtd>
                <m:mtd columnalign="left">
                  <m:mrow>
                    <m:munderover>
                      <m:mo>∑</m:mo>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mo>=</m:mo>
                        <m:msub>
                          <m:mi>j</m:mi>
                          <m:mn>1</m:mn>
                        </m:msub>
                      </m:mrow>
                      <m:mi>∞</m:mi>
                    </m:munderover>
                    <m:munder>
                      <m:mo>∑</m:mo>
                      <m:mi>k</m:mi>
                    </m:munder>
                    <m:msubsup>
                      <m:mi>d</m:mi>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mi>k</m:mi>
                      </m:mrow>
                      <m:mrow>
                        <m:mo>∘</m:mo>
                        <m:mspace width="4pt"/>
                        <m:mn>2</m:mn>
                      </m:mrow>
                    </m:msubsup>
                    <m:mo>=</m:mo>
                    <m:msub>
                      <m:mi>S</m:mi>
                      <m:mn>1</m:mn>
                    </m:msub>
                    <m:mo>+</m:mo>
                    <m:msub>
                      <m:mi>S</m:mi>
                      <m:mn>2</m:mn>
                    </m:msub>
                    <m:mo>+</m:mo>
                    <m:msub>
                      <m:mi>S</m:mi>
                      <m:mn>3</m:mn>
                    </m:msub>
                    <m:mspace width="3.33333pt"/>
                    <m:mo>,</m:mo>
                  </m:mrow>
                </m:mtd>
              </m:mtr>
            </m:mtable>
          </m:math>
        </equation>
        <para id="id2256140">where</para>
        <equation id="uid11">
          <m:math mode="display">
            <m:mrow>
              <m:msubsup>
                <m:mi>s</m:mi>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mi>k</m:mi>
                </m:mrow>
                <m:mo>∘</m:mo>
              </m:msubsup>
              <m:mo>:</m:mo>
              <m:mo>=</m:mo>
              <m:mfenced separators="" open="〈" close="〉">
                <m:mi>m</m:mi>
                <m:mspace width="0.166667em"/>
                <m:mo>,</m:mo>
                <m:msub>
                  <m:mi>ϕ</m:mi>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
              </m:mfenced>
              <m:mspace width="1.em"/>
              <m:mtext>and</m:mtext>
              <m:mspace width="1.em"/>
              <m:msubsup>
                <m:mi>d</m:mi>
                <m:mrow>
                  <m:mi>j</m:mi>
                  <m:mi>k</m:mi>
                </m:mrow>
                <m:mo>∘</m:mo>
              </m:msubsup>
              <m:mo>=</m:mo>
              <m:mfenced separators="" open="〈" close="〉">
                <m:mi>m</m:mi>
                <m:mspace width="0.166667em"/>
                <m:mo>,</m:mo>
                <m:msub>
                  <m:mi>ψ</m:mi>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
              </m:mfenced>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2256254">are called `theoretical' coefficients in the regression context.
The term <m:math><m:mrow><m:msub><m:mi>S</m:mi><m:mn>1</m:mn></m:msub><m:mo>+</m:mo><m:msub><m:mi>S</m:mi><m:mn>2</m:mn></m:msub></m:mrow></m:math> in <link target-id="uid10"/> constitutes the stochastic  bias whereas <m:math><m:msub><m:mi>S</m:mi><m:mn>3</m:mn></m:msub></m:math> is the deterministic  bias. The optimal cutting level is such that these two bias are of the same order.
If <m:math><m:mi>m</m:mi></m:math> is <m:math><m:mrow><m:mi>β</m:mi><m:mo>-</m:mo></m:mrow></m:math>Hölder continuous, it is easy to see that the optimal
cutting level is <m:math><m:mrow><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub><m:mrow><m:mo>(</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:mi>O</m:mi><m:mrow><m:mo>(</m:mo><m:msup><m:mi>n</m:mi><m:mrow><m:mn>1</m:mn><m:mo>/</m:mo><m:mo>(</m:mo><m:mn>1</m:mn><m:mo>+</m:mo><m:mn>2</m:mn><m:mi>β</m:mi><m:mo>)</m:mo></m:mrow></m:msup><m:mo>)</m:mo></m:mrow></m:mrow></m:math>. The resulting optimal IMSE is of
order <m:math><m:msup><m:mi>n</m:mi><m:mrow><m:mo>-</m:mo><m:mfrac><m:mrow><m:mn>2</m:mn><m:mi>β</m:mi></m:mrow><m:mrow><m:mn>2</m:mn><m:mi>β</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:mfrac></m:mrow></m:msup></m:math>.
In practice, cross-validation methods are often used to determine the optimal level <m:math><m:msub><m:mi>j</m:mi><m:mn>1</m:mn></m:msub></m:math> <link target-id="bid1"/>, <link target-id="bid2"/>.</para>
      </section>
      <section id="uid14">
        <title>Nonlinear smoothing with wavelets</title>
        <section id="uid15">
          <title>Hard-, soft-thresholding and wavelet estimator</title>
          <para id="id2256473">Given the regression model <link target-id="uid1"/>, we can decompose the empirical detail coefficient <m:math><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> in <link target-id="uid6"/> as</para>
          <equation id="uid16">
            <m:math mode="display">
              <m:mtable displaystyle="true">
                <m:mtr>
                  <m:mtd columnalign="right">
                    <m:msub>
                      <m:mover accent="true">
                        <m:mi>d</m:mi>
                        <m:mo>^</m:mo>
                      </m:mover>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mi>k</m:mi>
                      </m:mrow>
                    </m:msub>
                  </m:mtd>
                  <m:mtd>
                    <m:mo>=</m:mo>
                  </m:mtd>
                  <m:mtd columnalign="left">
                    <m:mrow>
                      <m:mfrac>
                        <m:mn>1</m:mn>
                        <m:mi>n</m:mi>
                      </m:mfrac>
                      <m:munderover>
                        <m:mo>∑</m:mo>
                        <m:mrow>
                          <m:mi>i</m:mi>
                          <m:mo>=</m:mo>
                          <m:mn>1</m:mn>
                        </m:mrow>
                        <m:mi>n</m:mi>
                      </m:munderover>
                      <m:mi>m</m:mi>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:msub>
                          <m:mi>x</m:mi>
                          <m:mi>i</m:mi>
                        </m:msub>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:msub>
                        <m:mi>ψ</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:msub>
                          <m:mi>x</m:mi>
                          <m:mi>i</m:mi>
                        </m:msub>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:mo>+</m:mo>
                      <m:mfrac>
                        <m:mn>1</m:mn>
                        <m:mi>n</m:mi>
                      </m:mfrac>
                      <m:munderover>
                        <m:mo>∑</m:mo>
                        <m:mrow>
                          <m:mi>i</m:mi>
                          <m:mo>=</m:mo>
                          <m:mn>1</m:mn>
                        </m:mrow>
                        <m:mi>n</m:mi>
                      </m:munderover>
                      <m:msub>
                        <m:mi>ϵ</m:mi>
                        <m:mi>i</m:mi>
                      </m:msub>
                      <m:msub>
                        <m:mi>ψ</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:msub>
                          <m:mi>x</m:mi>
                          <m:mi>i</m:mi>
                        </m:msub>
                        <m:mo>)</m:mo>
                      </m:mrow>
                    </m:mrow>
                  </m:mtd>
                </m:mtr>
                <m:mtr>
                  <m:mtd/>
                  <m:mtd>
                    <m:mo>=</m:mo>
                  </m:mtd>
                  <m:mtd columnalign="left">
                    <m:mrow>
                      <m:msub>
                        <m:mi>d</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mo>+</m:mo>
                      <m:msub>
                        <m:mi>ρ</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                    </m:mrow>
                  </m:mtd>
                </m:mtr>
              </m:mtable>
            </m:math>
          </equation>
          <para id="id2256707">If the function <m:math><m:mrow><m:mi>m</m:mi><m:mo>(</m:mo><m:mi>x</m:mi><m:mo>)</m:mo></m:mrow></m:math> allows for a  sparse wavelet representation, only
a few number of detail coefficients <m:math><m:msub><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> contribute to the signal and are
non-negligible. However, every empirical coefficient <m:math><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> has a non-zero
contribution coming from the noise part <m:math><m:msub><m:mi>ρ</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math>.</para>
          <note id="id2256795" type="Remark"><label>Remark</label> 
Note the link between the coefficients <m:math><m:msub><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> in <link target-id="uid16"/> and the theoretical coefficients <m:math><m:msubsup><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow><m:mo>∘</m:mo></m:msubsup></m:math> in <link target-id="uid11"/>:</note>
          <equation id="id2256852">
            <m:math mode="display">
              <m:mtable displaystyle="true">
                <m:mtr>
                  <m:mtd columnalign="right">
                    <m:msub>
                      <m:mi>d</m:mi>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mi>k</m:mi>
                      </m:mrow>
                    </m:msub>
                  </m:mtd>
                  <m:mtd>
                    <m:mo>=</m:mo>
                  </m:mtd>
                  <m:mtd columnalign="left">
                    <m:mrow>
                      <m:mfrac>
                        <m:mn>1</m:mn>
                        <m:mi>n</m:mi>
                      </m:mfrac>
                      <m:munderover>
                        <m:mo>∑</m:mo>
                        <m:mrow>
                          <m:mi>i</m:mi>
                          <m:mo>=</m:mo>
                          <m:mn>1</m:mn>
                        </m:mrow>
                        <m:mi>n</m:mi>
                      </m:munderover>
                      <m:mi>m</m:mi>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:msub>
                          <m:mi>x</m:mi>
                          <m:mi>i</m:mi>
                        </m:msub>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:msub>
                        <m:mi>ψ</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mo>,</m:mo>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:msub>
                          <m:mi>x</m:mi>
                          <m:mi>i</m:mi>
                        </m:msub>
                        <m:mo>)</m:mo>
                      </m:mrow>
                    </m:mrow>
                  </m:mtd>
                </m:mtr>
                <m:mtr>
                  <m:mtd/>
                  <m:mtd>
                    <m:mo>=</m:mo>
                  </m:mtd>
                  <m:mtd columnalign="left">
                    <m:mrow>
                      <m:mo>∫</m:mo>
                      <m:mspace width="-0.166667em"/>
                      <m:mi>m</m:mi>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:mi>x</m:mi>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:msub>
                        <m:mi>ψ</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mrow>
                        <m:mo>(</m:mo>
                        <m:mi>x</m:mi>
                        <m:mo>)</m:mo>
                      </m:mrow>
                      <m:mi>d</m:mi>
                      <m:mi>x</m:mi>
                      <m:mo>+</m:mo>
                      <m:mi>O</m:mi>
                      <m:mfenced separators="" open="(" close=")">
                        <m:mfrac>
                          <m:mn>1</m:mn>
                          <m:mi>n</m:mi>
                        </m:mfrac>
                      </m:mfenced>
                      <m:mo>=</m:mo>
                      <m:msubsup>
                        <m:mi>d</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                        <m:mo>∘</m:mo>
                      </m:msubsup>
                      <m:mo>+</m:mo>
                      <m:mi>O</m:mi>
                      <m:mfenced separators="" open="(" close=")">
                        <m:mfrac>
                          <m:mn>1</m:mn>
                          <m:mi>n</m:mi>
                        </m:mfrac>
                      </m:mfenced>
                      <m:mspace width="3.33333pt"/>
                      <m:mo>.</m:mo>
                    </m:mrow>
                  </m:mtd>
                </m:mtr>
              </m:mtable>
            </m:math>
          </equation>
          <para id="id2257055">In words, <m:math><m:msub><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> constitutes a first order approximation (using the trapezoidal rule) of the integral <m:math><m:msubsup><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow><m:mo>∘</m:mo></m:msubsup></m:math>. 
For the scaling coefficients <m:math><m:msubsup><m:mi>s</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow><m:mo>∘</m:mo></m:msubsup></m:math>, it can be proved <link target-id="bid3"/> that the order of accuracy of the trapezoidal rule is equal to <m:math><m:mrow><m:mi>N</m:mi><m:mo>-</m:mo><m:mn>1</m:mn></m:mrow></m:math>, where <m:math><m:mi>N</m:mi></m:math> is the order of the MRA associated to the scaling function.</para>
          <para id="id2257175">
Suppose the noise level is not too high, so that the signal can be distinguished from the noise.
Then, from the sparsity property of the wavelet, only the largest detail coefficients should be included in the wavelet estimator.
Hence, when estimating an unknown function, it makes sense to include only those coefficients that are larger than some specified threshold value <m:math><m:mi>t</m:mi></m:math>:</para>
          <equation id="id2257196">
            <m:math mode="display">
              <m:mrow>
                <m:msub>
                  <m:mi>η</m:mi>
                  <m:mi>H</m:mi>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>d</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>t</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mo>=</m:mo>
                <m:msub>
                  <m:mover accent="true">
                    <m:mi>d</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:msub>
                  <m:mn>1</m:mn>
                  <m:mrow>
                    <m:mo>{</m:mo>
                    <m:mo>|</m:mo>
                    <m:msub>
                      <m:mover accent="true">
                        <m:mi>d</m:mi>
                        <m:mo>^</m:mo>
                      </m:mover>
                      <m:mrow>
                        <m:mi>j</m:mi>
                        <m:mi>k</m:mi>
                      </m:mrow>
                    </m:msub>
                    <m:mo>|</m:mo>
                    <m:mo>&gt;</m:mo>
                    <m:mi>t</m:mi>
                    <m:mo>}</m:mo>
                  </m:mrow>
                </m:msub>
                <m:mspace width="3.33333pt"/>
                <m:mo>.</m:mo>
              </m:mrow>
            </m:math>
          </equation>
          <para id="id2257302">This `keep-or-kill' operation is called <emphasis>hard thresholding</emphasis>,  see <link target-id="uid22"/>(a).</para>
          <para id="id2257323">Now, since each empirical coefficient consists of both a signal part and a noise part, it may be desirable to shrink even the coefficients that are larger than the threshold:</para>
          <equation id="id2257329">
            <m:math mode="display">
              <m:mrow>
                <m:msubsup>
                  <m:mover accent="true">
                    <m:mi>d</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                  <m:mi>t</m:mi>
                </m:msubsup>
                <m:mo>:</m:mo>
                <m:mo>=</m:mo>
                <m:msub>
                  <m:mi>η</m:mi>
                  <m:mi>S</m:mi>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>d</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>t</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mo>=</m:mo>
                <m:mtext>sign</m:mtext>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>d</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:msub>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mo>|</m:mo>
                </m:mrow>
                <m:msub>
                  <m:mover accent="true">
                    <m:mi>d</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:msub>
                  <m:mrow>
                    <m:mo>|</m:mo>
                    <m:mo>-</m:mo>
                    <m:mi>t</m:mi>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>+</m:mo>
                </m:msub>
                <m:mspace width="3.33333pt"/>
                <m:mo>.</m:mo>
              </m:mrow>
            </m:math>
          </equation>
          <para id="id2257471">Since the function <m:math><m:msub><m:mi>η</m:mi><m:mi>S</m:mi></m:msub></m:math> is continuous in its first argument,
this procedure is called <emphasis>soft thresholding</emphasis>. 
More complex thresholding schemes have been proposed in the literature <link target-id="bid4"/>, <link target-id="bid5"/>, <link target-id="bid6"/>. They often appear as a compromise between soft and hard thresholding, see <link target-id="uid22"/>(b) for an example.</para>
          <figure id="uid22" orient="horizontal">
            <media id="id3495984" alt=""><image src="../../media/image3.png" mime-type="image/png" width="1207"/><image for="pdf" src="../../media/image3.eps" mime-type="application/postscript" print-width="6.75cm"/></media>
            <caption>In (a) the hard thresholding is represented in plain line: a coefficient <m:math><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math> with an absolute value below <m:math><m:mi>t</m:mi></m:math> is put equal to zero. The soft thresholding is given in dashed line: there coefficients with absolute value above the threshold <m:math><m:mi>t</m:mi></m:math> are shrunk of an amount equal to <m:math><m:mi>t</m:mi></m:math>.
In (b), a more complex thresholding procedure, the SCAD threshold devised in Antoniadis and Fan <link target-id="bid4"/> is represented.</caption>
          </figure>
          <para id="id2257606">For a given threshold value <m:math><m:mi>t</m:mi></m:math> and a thresholding scheme <m:math><m:msub><m:mi>η</m:mi><m:mrow><m:mo>(</m:mo><m:mo>.</m:mo><m:mo>)</m:mo></m:mrow></m:msub></m:math>, the nonlinear wavelet estimator is given by </para>
          <equation id="id2257643">
            <m:math mode="display">
              <m:mrow>
                <m:mover accent="true">
                  <m:mi>m</m:mi>
                  <m:mo>^</m:mo>
                </m:mover>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>x</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mo>=</m:mo>
                <m:munder>
                  <m:mo>∑</m:mo>
                  <m:mi>k</m:mi>
                </m:munder>
                <m:msub>
                  <m:mover accent="true">
                    <m:mi>s</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mrow>
                    <m:msub>
                      <m:mi>j</m:mi>
                      <m:mn>0</m:mn>
                    </m:msub>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:mspace width="4pt"/>
                <m:msub>
                  <m:mi>ϕ</m:mi>
                  <m:mrow>
                    <m:msub>
                      <m:mi>j</m:mi>
                      <m:mn>0</m:mn>
                    </m:msub>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>x</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mspace width="4pt"/>
                <m:mo>+</m:mo>
                <m:mspace width="4pt"/>
                <m:munder>
                  <m:mo>∑</m:mo>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mo>,</m:mo>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:munder>
                <m:msub>
                  <m:mi>η</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mo>.</m:mo>
                    <m:mo>)</m:mo>
                  </m:mrow>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>d</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:msub>
                  <m:mo>,</m:mo>
                  <m:mi>t</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mspace width="4pt"/>
                <m:msub>
                  <m:mi>ψ</m:mi>
                  <m:mrow>
                    <m:mi>j</m:mi>
                    <m:mi>k</m:mi>
                  </m:mrow>
                </m:msub>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mi>x</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mspace width="3.33333pt"/>
                <m:mo>,</m:mo>
              </m:mrow>
            </m:math>
          </equation>
          <para id="id2257817">where <m:math><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub></m:math> denotes the <emphasis>primary resolution level</emphasis>. It indicates the level above which the detail coefficients are being manipulated. </para>
          <para id="id2257848">Let now <m:math><m:mrow><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mi>j</m:mi></m:msub><m:mo>=</m:mo><m:mrow><m:mo>{</m:mo><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo>,</m:mo><m:mi>k</m:mi><m:mo>=</m:mo><m:mn>0</m:mn><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:msup><m:mn>2</m:mn><m:mi>j</m:mi></m:msup><m:mo>-</m:mo><m:mn>1</m:mn><m:mo>}</m:mo></m:mrow></m:mrow></m:math> denote the vector of empirical detail coefficients at level <m:math><m:mi>j</m:mi></m:math> and similarly define <m:math><m:msub><m:mover accent="true"><m:mi>s</m:mi><m:mo>^</m:mo></m:mover><m:mi>j</m:mi></m:msub></m:math>.
In practice a nonlinear wavelet estimator is obtained in three steps.</para>
          <list id="id2257959" list-type="enumerated">
            <item id="uid25">Apply the analyzing (forward) wavelet transform on the observations <m:math><m:msubsup><m:mrow><m:mo>{</m:mo><m:msub><m:mi>Y</m:mi><m:mi>i</m:mi></m:msub><m:mo>}</m:mo></m:mrow><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>n</m:mi></m:msubsup></m:math>, yielding <m:math><m:msub><m:mover accent="true"><m:mi>s</m:mi><m:mo>^</m:mo></m:mover><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub></m:msub></m:math> and <m:math><m:mrow><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mi>j</m:mi></m:msub><m:mo>,</m:mo></m:mrow></m:math> for
<m:math><m:mrow><m:mi>j</m:mi><m:mo>=</m:mo><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub><m:mo>,</m:mo><m:mo>...</m:mo><m:mo>,</m:mo><m:mi>J</m:mi><m:mo>-</m:mo><m:mn>1</m:mn></m:mrow></m:math>.
</item>
            <item id="uid26">Manipulate the detail coefficients above the level <m:math><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub></m:math>, e.g. by soft-thresholding them.
</item>
            <item id="uid27">Invert the wavelet transform and produce an estimation of <m:math><m:mi>m</m:mi></m:math> at the design points: <m:math><m:msubsup><m:mrow><m:mo>{</m:mo><m:mover accent="true"><m:mi>m</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mo>}</m:mo></m:mrow><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>n</m:mi></m:msubsup></m:math>.
</item>
          </list>
          <para id="id2258186">If necessary, a continuous estimator <m:math><m:mover accent="true"><m:mi>m</m:mi><m:mo>^</m:mo></m:mover></m:math> can then be constructed by an appropriate interpolation of the estimated <m:math><m:mrow><m:mover accent="true"><m:mi>m</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow></m:mrow></m:math> values <link target-id="bid7"/>.</para>
          <para id="id2258242">The choice of the primary resolution level in nonlinear wavelet estimation has the same 
importance as the choice of a particular kernel in local polynomial estimation, i.e., it is not the most important factor. It is common practice to take <m:math><m:mrow><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub><m:mo>=</m:mo><m:mn>2</m:mn></m:mrow></m:math> or <m:math><m:mrow><m:msub><m:mi>j</m:mi><m:mn>0</m:mn></m:msub><m:mo>=</m:mo><m:mn>3</m:mn></m:mrow></m:math>, although a cross-validation determination is of course possible <link target-id="bid2"/>.</para>
          <para id="id2258301">The selection of a  threshold value is much more crucial.
If it is chosen too large, the thresholding operation will kill too many coefficients. Too few coefficients will then be included in the reconstruction, resulting in an oversmoothed estimator. Conversely, a small threshold value will allow many coefficients to be included in the reconstruction, giving a rough, or undersmoothed estimator. A proper choice of the threshold involves thus a careful balance between smoothness and closeness of fit.</para>
          <para id="id2258317">In case of an orthogonal transform and i.i.d. white noise, the same threshold can be applied to all detail coefficients, since the errors in the wavelet domain are still i.i.d. white noise. However, if the errors are stationary but correlated, or if the transform is biorthogonal, a level-dependent threshold is necessary to obtain optimal results <link target-id="bid8"/>, <link target-id="bid9"/>. Finally, in the irregular setting, a level and location dependent threshold must be utilized.</para>
          <para id="id2258344">Many efforts have been devoted to propose methods for selecting the threshold. We now review some of the procedures encountered in the literature.</para>
        </section>
        <section id="uid30">
          <title>Choice of the threshold</title>
          <section id="uid31">
            <title>Universal threshold</title>
            <para id="id2258365">The most  simple method to find a threshold whose value is supported by some statistical arguments, is probably to use the so-called `universal threshold' <link target-id="bid7"/>, <link target-id="bid10"/></para>
            <equation id="id2258388">
              <m:math mode="display">
                <m:mrow>
                  <m:msub>
                    <m:mi>t</m:mi>
                    <m:mtext>univ</m:mtext>
                  </m:msub>
                  <m:mo>=</m:mo>
                  <m:msub>
                    <m:mi>σ</m:mi>
                    <m:mi>d</m:mi>
                  </m:msub>
                  <m:msqrt>
                    <m:mrow>
                      <m:mn>2</m:mn>
                      <m:mo form="prefix">log</m:mo>
                      <m:mi>n</m:mi>
                    </m:mrow>
                  </m:msqrt>
                  <m:mspace width="3.33333pt"/>
                  <m:mo>,</m:mo>
                </m:mrow>
              </m:math>
            </equation>
            <para id="id2258432">where the only quantity to be estimated is <m:math><m:msubsup><m:mi>σ</m:mi><m:mi>d</m:mi><m:mn>2</m:mn></m:msubsup></m:math>, which constitutes the variance of the empirical wavelet coefficients.
In case of an orthogonal transform, <m:math><m:mrow><m:msub><m:mi>σ</m:mi><m:mi>d</m:mi></m:msub><m:mo>=</m:mo><m:msub><m:mi>σ</m:mi><m:mi>ϵ</m:mi></m:msub><m:mo>/</m:mo><m:msqrt><m:mi>n</m:mi></m:msqrt></m:mrow></m:math>.</para>
            <para id="id2258489">In a wavelet transform, the detail coefficients at fine scales are, with a small fraction of exception, essentially pure noise. This is the reason why Donoho and Johnstone proposed in <link target-id="bid11"/> to estimate <m:math><m:msub><m:mi>σ</m:mi><m:mi>d</m:mi></m:msub></m:math> in a robust way using the median absolute deviation from the median (MAD) of <m:math><m:msub><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mi>J</m:mi><m:mo>-</m:mo><m:mn>1</m:mn></m:mrow></m:msub></m:math>:</para>
            <equation id="id2258547">
              <m:math mode="display">
                <m:mrow>
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>σ</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mi>d</m:mi>
                  </m:msub>
                  <m:mo>=</m:mo>
                  <m:mfrac>
                    <m:mrow>
                      <m:mtext>median</m:mtext>
                      <m:mfenced separators="" open="(" close=")">
                        <m:mfenced separators="" open="|" close="|">
                          <m:msub>
                            <m:mover accent="true">
                              <m:mi>d</m:mi>
                              <m:mo>^</m:mo>
                            </m:mover>
                            <m:mrow>
                              <m:mi>J</m:mi>
                              <m:mo>-</m:mo>
                              <m:mn>1</m:mn>
                            </m:mrow>
                          </m:msub>
                          <m:mo>-</m:mo>
                          <m:mtext>median</m:mtext>
                          <m:mrow>
                            <m:mo>(</m:mo>
                            <m:msub>
                              <m:mover accent="true">
                                <m:mi>d</m:mi>
                                <m:mo>^</m:mo>
                              </m:mover>
                              <m:mrow>
                                <m:mi>J</m:mi>
                                <m:mo>-</m:mo>
                                <m:mn>1</m:mn>
                              </m:mrow>
                            </m:msub>
                            <m:mo>)</m:mo>
                          </m:mrow>
                        </m:mfenced>
                      </m:mfenced>
                    </m:mrow>
                    <m:mrow>
                      <m:mn>0</m:mn>
                      <m:mo>.</m:mo>
                      <m:mn>6745</m:mn>
                    </m:mrow>
                  </m:mfrac>
                  <m:mspace width="3.33333pt"/>
                  <m:mo>.</m:mo>
                </m:mrow>
              </m:math>
            </equation>
            <para id="id2258656">If the universal threshold is used in conjunction with soft thresholding,
the resulting estimator possesses a noise-free property: with a high probability,
an appropriate interpolation of <m:math><m:mrow><m:mo>{</m:mo><m:mover accent="true"><m:mi>m</m:mi><m:mo>^</m:mo></m:mover><m:mrow><m:mo>(</m:mo><m:msub><m:mi>x</m:mi><m:mi>i</m:mi></m:msub><m:mo>)</m:mo></m:mrow><m:mo>}</m:mo></m:mrow></m:math> produces an estimator which is at least as smooth as the function <m:math><m:mi>m</m:mi></m:math>, see Theorem 1.1 in <link target-id="bid7"/>.
Hence the reconstruction is of good visual quality, so that Donoho and
Johnstone called the procedure `VisuShrink' <link target-id="bid11"/>. 
Although simple, this estimator enjoys a near-minimax adaptivity property, see <link target-id="uid51">"Adaptivity of wavelet estimator"</link>.
However, this near-optimality is an asymptotic one: for small sample size <m:math><m:msub><m:mi>t</m:mi><m:mtext>univ</m:mtext></m:msub></m:math> may be too large, leading to a poor mean square error.</para>
          </section>
          <section id="uid34">
            <title>Oracle inequality</title>
            <para id="id2258762">Consider  the soft-thresholded detail coefficients <m:math><m:msup><m:mover accent="true"><m:mi>d</m:mi><m:mo>^</m:mo></m:mover><m:mi>t</m:mi></m:msup></m:math>.
Another approach to find an optimal threshold is to look at the <m:math><m:mrow><m:msub><m:mi>l</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk</para>
            <equation id="uid36">
              <m:math mode="display">
                <m:mrow>
                  <m:mi mathvariant="script">R</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msup>
                      <m:mover accent="true">
                        <m:mi>d</m:mi>
                        <m:mo>^</m:mo>
                      </m:mover>
                      <m:mi>t</m:mi>
                    </m:msup>
                    <m:mo>,</m:mo>
                    <m:mi>d</m:mi>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>=</m:mo>
                  <m:mi>E</m:mi>
                  <m:munder>
                    <m:mo>∑</m:mo>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>j</m:mi>
                      <m:mo>,</m:mo>
                      <m:mi>k</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                  </m:munder>
                  <m:msup>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:msubsup>
                        <m:mover accent="true">
                          <m:mi>d</m:mi>
                          <m:mo>^</m:mo>
                        </m:mover>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                        <m:mi>t</m:mi>
                      </m:msubsup>
                      <m:mo>-</m:mo>
                      <m:msub>
                        <m:mi>d</m:mi>
                        <m:mrow>
                          <m:mi>j</m:mi>
                          <m:mi>k</m:mi>
                        </m:mrow>
                      </m:msub>
                      <m:mo>)</m:mo>
                    </m:mrow>
                    <m:mn>2</m:mn>
                  </m:msup>
                  <m:mo>=</m:mo>
                  <m:mi>E</m:mi>
                  <m:msubsup>
                    <m:mfenced separators="" open="∥" close="∥">
                      <m:msup>
                        <m:mover accent="true">
                          <m:mi>d</m:mi>
                          <m:mo>^</m:mo>
                        </m:mover>
                        <m:mi>t</m:mi>
                      </m:msup>
                      <m:mo>-</m:mo>
                      <m:mi>d</m:mi>
                    </m:mfenced>
                    <m:msub>
                      <m:mi>l</m:mi>
                      <m:mn>2</m:mn>
                    </m:msub>
                    <m:mn>2</m:mn>
                  </m:msubsup>
                  <m:mspace width="3.33333pt"/>
                  <m:mo>,</m:mo>
                </m:mrow>
              </m:math>
            </equation>
            <para id="id2258967">and to relate this risk with the one of an ideal risk <m:math><m:msub><m:mi mathvariant="script">R</m:mi><m:mtext>ideal</m:mtext></m:msub></m:math>. The ideal risk is the risk obtained if an oracle tells us exactly which coefficients to keep or to kill.</para>
            <para id="id2258992">In <link target-id="bid10"/>, Donoho and Johnstone showed that, when using the universal threshold, the following oracle inequality prevails</para>
            <equation id="id2259003">
              <m:math mode="display">
                <m:mrow>
                  <m:mi mathvariant="script">R</m:mi>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:msup>
                      <m:mover accent="true">
                        <m:mi>d</m:mi>
                        <m:mo>^</m:mo>
                      </m:mover>
                      <m:mi>t</m:mi>
                    </m:msup>
                    <m:mo>,</m:mo>
                    <m:mi>d</m:mi>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mo>≤</m:mo>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mn>2</m:mn>
                    <m:mo form="prefix">log</m:mo>
                    <m:mi>n</m:mi>
                    <m:mo>+</m:mo>
                    <m:mn>1</m:mn>
                    <m:mo>)</m:mo>
                  </m:mrow>
                  <m:mfenced separators="" open="(" close=")">
                    <m:mrow>
                      <m:mfrac>
                        <m:msubsup>
                          <m:mi>σ</m:mi>
                          <m:mi>ϵ</m:mi>
                          <m:mn>2</m:mn>
                        </m:msubsup>
                        <m:mi>n</m:mi>
                      </m:mfrac>
                      <m:mo>+</m:mo>
                      <m:msub>
                        <m:mi mathvariant="script">R</m:mi>
                        <m:mtext>ideal</m:mtext>
                      </m:msub>
                    </m:mrow>
                  </m:mfenced>
                  <m:mspace width="3.33333pt"/>
                  <m:mo>.</m:mo>
                </m:mrow>
              </m:math>
            </equation>
            <para id="id2259103">However, this inequality is not optimal. Donoho and Johnstone looked for the optimal threshold <m:math><m:mrow><m:msup><m:mi>t</m:mi><m:mo>*</m:mo></m:msup><m:mrow><m:mo>(</m:mo><m:mi>n</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> which leads to the smallest possible constant <m:math><m:msubsup><m:mi>Λ</m:mi><m:mi>n</m:mi><m:mo>*</m:mo></m:msubsup></m:math> in place of <m:math><m:mrow><m:mn>2</m:mn><m:mo form="prefix">log</m:mo><m:mi>n</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:math>. Such a threshold does not exist in closed form, but can be approximated numerically. For small sample size, it is sensibly smaller than the universal threshold.</para>
          </section>
          <section id="uid37">
            <title>SureShrink procedure</title>
            <para id="id2259184">Given  the expression <link target-id="uid36"/> for the <m:math><m:msub><m:mi>l</m:mi><m:mn>2</m:mn></m:msub></m:math>-risk, it is natural to look for a threshold that minimizes an estimation of this risk.</para>
            <para id="id2259213">By minimizing Stein's unbiased estimate of the risk <link target-id="bid12"/> and using a soft thresholding scheme, the resulting estimator, called `SureShrink', is
adaptive over a wide range of function spaces including Hölder, Sobolev, and Besov spaces, see <link target-id="uid51">"Adaptivity of wavelet estimator"</link>. That is, without any a priori knowledge on the type or amount of regularity of the function, the SURE procedure nevertheless achieves the optimal rate of convergence that one could attain by knowing the regularity of the function.</para>
          </section>
          <section id="uid39">
            <title>Other thresholding procedures</title>
            <para id="id2259246">We mention some of the other thresholding or shrinkage procedures proposed in the literature.</para>
            <para id="id2259251">Instead of considering each coefficient individually, Cai
<emphasis>et al.</emphasis> <link target-id="bid13"/>, <link target-id="bid14"/> consider blocks
of empirical wavelet coefficients in order to make simultaneous shrinkage
decisions about all coefficients within a block.</para>
            <para id="id2259275">Another fruitful idea is to use the Bayesian framework. 
There a prior distribution is imposed on the wavelet coefficients <m:math><m:msub><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub></m:math>. This prior model is designed to capture the sparseness of the wavelet expansion.
Next, the function is estimated by applying some Bayes rules on the resulting posterior
distribution of the wavelet coefficients, see for example <link target-id="bid15"/>, <link target-id="bid16"/>, <link target-id="bid17"/>, <link target-id="bid18"/>.</para>
            <para id="id2259332">Antoniadis and Fan <link target-id="bid4"/> treat the problem of selecting the wavelet coefficients as a penalized least squares issue.
Let <m:math><m:mi>W</m:mi></m:math> be the matrix of an orthogonal wavelet transform
and <m:math><m:mrow><m:mi>Y</m:mi><m:mo>:</m:mo><m:mo>=</m:mo><m:msubsup><m:mrow><m:mo>{</m:mo><m:msub><m:mi>Y</m:mi><m:mi>i</m:mi></m:msub><m:mo>}</m:mo></m:mrow><m:mrow><m:mi>i</m:mi><m:mo>=</m:mo><m:mn>1</m:mn></m:mrow><m:mi>n</m:mi></m:msubsup></m:mrow></m:math>.
The detail coefficients <m:math><m:mrow><m:mi>d</m:mi><m:mo>:</m:mo><m:mo>=</m:mo><m:mo>{</m:mo><m:msub><m:mi>d</m:mi><m:mrow><m:mi>j</m:mi><m:mi>k</m:mi></m:mrow></m:msub><m:mo>}</m:mo></m:mrow></m:math> which minimize</para>
            <equation id="uid41">
              <m:math mode="display">
                <m:mrow>
                  <m:msubsup>
                    <m:mfenced separators="" open="∥" close="∥">
                      <m:mi>W</m:mi>
                      <m:mi>Y</m:mi>
                      <m:mo>-</m:mo>
                      <m:mi>d</m:mi>
                    </m:mfenced>
                    <m:msub>
                      <m:mi>l</m:mi>
                      <m:mn>2</m:mn>
                    </m:msub>
                    <m:mn>2</m:mn>
                  </m:msubsup>
                  <m:mo>+</m:mo>
                  <m:munder>
                    <m:mo>∑</m:mo>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mo>,</m:mo>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:munder>
                  <m:msub>
                    <m:mi>p</m:mi>
                    <m:mi>λ</m:mi>
                  </m:msub>
                  <m:mrow>
                    <m:mo>(</m:mo>
                    <m:mo>|</m:mo>
                  </m:mrow>
                  <m:msub>
                    <m:mi>d</m:mi>
                    <m:mrow>
                      <m:mi>j</m:mi>
                      <m:mi>k</m:mi>
                    </m:mrow>
                  </m:msub>
                  <m:mrow>
                    <m:mo>|</m:mo>
                    <m:mo>)</m:mo>
                  </m:mrow>
                </m:mrow>
              </m:math>
            </equation>
            <para id="id2259511">are used to estimate the true wavelet coefficients.
In equation <link target-id="uid41"/>, <m:math><m:mrow><m:msub><m:mi>p</m:mi><m:mi>λ</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mo>·</m:mo><m:mo>)</m:mo></m:mrow></m:mrow></m:math> is a penalty function which depends
on the regularization parameter <m:math><m:mi>λ</m:mi></m:math>.
The authors provide a general framework, where
different penalty functions correspond to different type of thresholding procedures
(like, e.g., the soft- and hard- thresholding)
and obtain oracle inequalities for a large class of penalty functions.</para>
            <para id="id2259561">Other methods include threshold selection by hypothesis testing <link target-id="bid19"/>, cross-validation <link target-id="bid20"/>, or generalized
cross-validation <link target-id="bid21"/>, <link target-id="bid22"/>, which is used to estimated the <m:math><m:msub><m:mi>l</m:mi><m:mn>2</m:mn></m:msub></m:math>-risk of the empirical detail coefficients.</para>
          </section>
        </section>
      </section>
      <section id="uid42">
        <title>Linear versus nonlinear wavelet estimator</title>
        <para id="id2259614">In order to differenciate the behaviours of a linear and of a nonlinear wavelet estimator, we consider the Sobolev  class <m:math><m:mrow><m:msubsup><m:mi>W</m:mi><m:mi>q</m:mi><m:mi>s</m:mi></m:msubsup><m:mrow><m:mo>(</m:mo><m:mi>C</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> defined as</para>
        <equation id="uid44">
          <m:math mode="display">
            <m:mrow>
              <m:msubsup>
                <m:mi>W</m:mi>
                <m:mi>q</m:mi>
                <m:mi>s</m:mi>
              </m:msubsup>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>C</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:mrow>
                <m:mo>{</m:mo>
                <m:mi>f</m:mi>
                <m:mo>:</m:mo>
                <m:msubsup>
                  <m:mfenced open="∥" close="∥">
                    <m:mi>f</m:mi>
                  </m:mfenced>
                  <m:mi>q</m:mi>
                  <m:mi>q</m:mi>
                </m:msubsup>
                <m:mo>+</m:mo>
                <m:msubsup>
                  <m:mfenced separators="" open="∥" close="∥">
                    <m:mfrac>
                      <m:msup>
                        <m:mi>d</m:mi>
                        <m:mi>s</m:mi>
                      </m:msup>
                      <m:mrow>
                        <m:mi>d</m:mi>
                        <m:msup>
                          <m:mi>x</m:mi>
                          <m:mi>s</m:mi>
                        </m:msup>
                      </m:mrow>
                    </m:mfrac>
                    <m:mi>f</m:mi>
                    <m:mrow>
                      <m:mo>(</m:mo>
                      <m:mi>x</m:mi>
                      <m:mo>)</m:mo>
                    </m:mrow>
                  </m:mfenced>
                  <m:mi>q</m:mi>
                  <m:mi>q</m:mi>
                </m:msubsup>
                <m:mo>≤</m:mo>
                <m:msup>
                  <m:mi>C</m:mi>
                  <m:mn>2</m:mn>
                </m:msup>
                <m:mo>}</m:mo>
              </m:mrow>
              <m:mspace width="3.33333pt"/>
              <m:mo>,</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2259774">and that we denote <m:math><m:mi>V</m:mi></m:math> in short.
Assume we know that <m:math><m:mi>m</m:mi></m:math>, the function to be estimated, belongs to <m:math><m:mi>V</m:mi></m:math>. In the next section, we will release this assumption.
The <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk
 of an arbitrary estimator <m:math><m:msub><m:mi>T</m:mi><m:mi>n</m:mi></m:msub></m:math> based on the sample data is defined as <m:math><m:mrow><m:mi>E</m:mi><m:msubsup><m:mfenced separators="" open="∥" close="∥"><m:msub><m:mi>T</m:mi><m:mi>n</m:mi></m:msub><m:mo>-</m:mo><m:mi>m</m:mi></m:mfenced><m:mi>p</m:mi><m:mi>p</m:mi></m:msubsup><m:mo>,</m:mo><m:mspace width="0.166667em"/><m:mn>1</m:mn><m:mo>≤</m:mo><m:mi>p</m:mi><m:mo>&lt;</m:mo><m:mi>∞</m:mi></m:mrow></m:math>, whereas the
<m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>minimax risk
 is given by </para>
        <equation id="id2259927">
          <m:math mode="display">
            <m:mrow>
              <m:msub>
                <m:mi>R</m:mi>
                <m:mi>n</m:mi>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>V</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">inf</m:mo>
                <m:msub>
                  <m:mi>T</m:mi>
                  <m:mi>n</m:mi>
                </m:msub>
              </m:munder>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi>V</m:mi>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msub>
                    <m:mi>T</m:mi>
                    <m:mi>n</m:mi>
                  </m:msub>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>,</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2260026">where the infimum is taken over all measurable estimators <m:math><m:msub><m:mi>T</m:mi><m:mi>n</m:mi></m:msub></m:math> of <m:math><m:mrow><m:mi>m</m:mi><m:mo>.</m:mo></m:mrow></m:math>
Similarly, we define the linear <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>minimax risk
 as</para>
        <equation id="id2260083">
          <m:math mode="display">
            <m:mrow>
              <m:msubsup>
                <m:mi>R</m:mi>
                <m:mrow>
                  <m:mi>n</m:mi>
                </m:mrow>
                <m:mtext>lin</m:mtext>
              </m:msubsup>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>V</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">inf</m:mo>
                <m:msubsup>
                  <m:mi>T</m:mi>
                  <m:mi>n</m:mi>
                  <m:mtext>lin</m:mtext>
                </m:msubsup>
              </m:munder>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi>V</m:mi>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msubsup>
                    <m:mi>T</m:mi>
                    <m:mi>n</m:mi>
                    <m:mtext>lin</m:mtext>
                  </m:msubsup>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>,</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2260192">where the infimum is now taken over all <emphasis>linear</emphasis> estimators <m:math><m:mrow><m:msubsup><m:mi>T</m:mi><m:mi>n</m:mi><m:mtext>lin</m:mtext></m:msubsup><m:mo>.</m:mo></m:mrow></m:math> Obviously, <m:math><m:mrow><m:msubsup><m:mi>R</m:mi><m:mrow><m:mi>n</m:mi></m:mrow><m:mtext>lin</m:mtext></m:msubsup><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mi>p</m:mi><m:mo>)</m:mo></m:mrow><m:mo>≥</m:mo><m:msub><m:mi>R</m:mi><m:mi>n</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mi>p</m:mi><m:mo>)</m:mo></m:mrow><m:mo>.</m:mo></m:mrow></m:math> We first state some definitions.</para>
        <note id="id2260279" type="Definition"><label>Definition</label>

The sequences <m:math><m:mrow><m:mo>{</m:mo><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>}</m:mo></m:mrow></m:math> and <m:math><m:mrow><m:mo>{</m:mo><m:msub><m:mi>b</m:mi><m:mi>n</m:mi></m:msub><m:mo>}</m:mo></m:mrow></m:math> are said to be <emphasis>asymptotically equivalent</emphasis> and are noted <m:math><m:mrow><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>∼</m:mo><m:msub><m:mi>b</m:mi><m:mi>n</m:mi></m:msub></m:mrow></m:math> if the ratio <m:math><m:mrow><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>/</m:mo><m:msub><m:mi>b</m:mi><m:mi>n</m:mi></m:msub></m:mrow></m:math> is bounded away from zero and <m:math><m:mi>∞</m:mi></m:math> as <m:math><m:mrow><m:mi>n</m:mi><m:mo>→</m:mo><m:mi>∞</m:mi><m:mo>.</m:mo></m:mrow></m:math></note>
        <note id="id2260412" type="Definition"><label>Definition</label>The sequence <m:math><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub></m:math> is called optimal  rate of convergence
, (or <emphasis>minimax rate of convergence</emphasis>) on the class <m:math><m:mi>V</m:mi></m:math> for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk if <m:math><m:mrow><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>∼</m:mo><m:msub><m:mi>R</m:mi><m:mi>n</m:mi></m:msub><m:msup><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mi>p</m:mi><m:mo>)</m:mo></m:mrow><m:mrow><m:mn>1</m:mn><m:mo>/</m:mo><m:mi>p</m:mi></m:mrow></m:msup></m:mrow></m:math>. We say that an estimator <m:math><m:msub><m:mi>m</m:mi><m:mi>n</m:mi></m:msub></m:math> of <m:math><m:mi>m</m:mi></m:math> attains the optimal rate of convergence if 
          <m:math mode="display">
            <m:mrow>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:mi>V</m:mi>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msub>
                    <m:mi>m</m:mi>
                    <m:mi>n</m:mi>
                  </m:msub>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>∼</m:mo>
              <m:msub>
                <m:mi>R</m:mi>
                <m:mi>n</m:mi>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>V</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </note>
        
        <para id="id2260631">In order to fix the idea, we consider only the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk in the remaining part of this section, thus <m:math><m:mrow><m:mi>p</m:mi><m:mo>:</m:mo><m:mo>=</m:mo><m:mn>2</m:mn></m:mrow></m:math>.</para>
        <para id="id2260670">In <link target-id="bid23"/>, <link target-id="bid24"/>, the authors found that the optimal rate of convergence attainable by an estimator when the underlying function belongs to the Sobolev class <m:math><m:msubsup><m:mi>W</m:mi><m:mi>q</m:mi><m:mi>s</m:mi></m:msubsup></m:math> is <m:math><m:mrow><m:msub><m:mi>a</m:mi><m:mi>n</m:mi></m:msub><m:mo>=</m:mo><m:msup><m:mi>n</m:mi><m:mfrac><m:mrow><m:mo>-</m:mo><m:mi>s</m:mi></m:mrow><m:mrow><m:mn>2</m:mn><m:mi>s</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:mfrac></m:msup></m:mrow></m:math>, hence <m:math><m:mrow><m:msub><m:mi>R</m:mi><m:mi>n</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mn>2</m:mn><m:mo>)</m:mo></m:mrow><m:mo>=</m:mo><m:msup><m:mi>n</m:mi><m:mfrac><m:mrow><m:mo>-</m:mo><m:mn>2</m:mn><m:mi>s</m:mi></m:mrow><m:mrow><m:mn>2</m:mn><m:mi>s</m:mi><m:mo>+</m:mo><m:mn>1</m:mn></m:mrow></m:mfrac></m:msup></m:mrow></m:math>.
We saw in <link target-id="uid2">"Linear smoothing with wavelets"</link> that linear wavelet estimators attain the optimal rate for <m:math><m:mrow><m:mi>s</m:mi><m:mo>-</m:mo></m:mrow></m:math>Hölder function in case of the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>risk (also called `IMSE'). For a Sobolev class <m:math><m:msubsup><m:mi>W</m:mi><m:mi>q</m:mi><m:mi>s</m:mi></m:msubsup></m:math>, the same result holds provided that <m:math><m:mrow><m:mi>q</m:mi><m:mo>≥</m:mo><m:mn>2</m:mn></m:mrow></m:math>. More precisely, we have the two following situations.</para>
        <list id="id2260869" list-type="enumerated">
          <item id="uid49">If <m:math><m:mrow><m:mi>q</m:mi><m:mo>≥</m:mo><m:mn>2</m:mn><m:mo>,</m:mo></m:mrow></m:math> we are in the so-called <emphasis>homogeneous</emphasis> zone. In this zone of spatial homogeneity, linear estimators can attain the optimal rate of convergence <m:math><m:mrow><m:msup><m:mi>n</m:mi><m:mrow><m:mo>-</m:mo><m:mi>s</m:mi><m:mo>/</m:mo><m:mo>(</m:mo><m:mn>2</m:mn><m:mi>s</m:mi><m:mo>+</m:mo><m:mn>1</m:mn><m:mo>)</m:mo></m:mrow></m:msup><m:mo>.</m:mo></m:mrow></m:math></item>
          <item id="uid50">If <m:math><m:mrow><m:mi>q</m:mi><m:mo>&lt;</m:mo><m:mn>2</m:mn><m:mo>,</m:mo></m:mrow></m:math> we are in the <emphasis>non-homogeneous</emphasis> zone, where linear estimators do not attain the optimal rate of convergence. Instead, we have:
<equation id="id2260975"><m:math mode="display"><m:mrow><m:msubsup><m:mi>R</m:mi><m:mi>n</m:mi><m:mtext>lin</m:mtext></m:msubsup><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mn>2</m:mn><m:mo>)</m:mo></m:mrow><m:mo>/</m:mo><m:msub><m:mi>R</m:mi><m:mi>n</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>V</m:mi><m:mo>,</m:mo><m:mn>2</m:mn><m:mo>)</m:mo></m:mrow><m:mo>→</m:mo><m:mi>∞</m:mi><m:mo>,</m:mo><m:mspace width="4.pt"/><m:mtext>as</m:mtext><m:mspace width="4.pt"/><m:mi>n</m:mi><m:mo>→</m:mo><m:mi>∞</m:mi><m:mo>.</m:mo></m:mrow></m:math></equation></item>
        </list>
        <para id="id2261055">The second result is due to the spatial variability of functions in Sobolev spaces with small index <m:math><m:mi>q</m:mi></m:math>. Linear estimators are based on the idea of spatial homogeneity of the function and hence do perform poorly in the presence of non-homogeneous functions.
In contrast, even if <m:math><m:mrow><m:mi>q</m:mi><m:mo>&lt;</m:mo><m:mn>2</m:mn></m:mrow></m:math>, the SureShrink estimator attains the minimax rate <link target-id="bid11"/>. The same type of results holds for more general Besov spaces, see for example <link target-id="bid0"/>, Chapter 10.</para>
      </section>
      <section id="uid51">
        <title>Adaptivity of wavelet estimator</title>
        <para id="id2261110">We just saw 
that a nonlinear wavelet estimator is able to estimate in an optimal way functions of 
inhomogeneous regularity. However, it may not be sufficient to know that for <m:math><m:mi>m</m:mi></m:math> belonging to a given space, the estimator performs well. Indeed,
in general we do not know which space the function belongs to. Hence it is of
great interest to consider a <emphasis>scale</emphasis> of function  classes and to look for
an estimator that attains <emphasis>simultaneously</emphasis>
the best rates of convergence across
the whole scale. For example, the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>q</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>Sobolev scale is a set of Sobolev function classes <m:math><m:mrow><m:msubsup><m:mi>W</m:mi><m:mi>q</m:mi><m:mi>s</m:mi></m:msubsup><m:mrow><m:mo>(</m:mo><m:mi>C</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> indexed by the parameters <m:math><m:mi>s</m:mi></m:math> and <m:math><m:mi>C</m:mi></m:math>,
see <link target-id="uid44"/> for the definition of a Sobolev class.  We now formalize the notion of an adaptive estimator.</para>
        <para id="id2261229">Let <m:math><m:mi>A</m:mi></m:math> be a given set and let <m:math><m:mrow><m:mo>{</m:mo><m:msub><m:mi mathvariant="script">F</m:mi><m:mi>α</m:mi></m:msub><m:mo>,</m:mo><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi><m:mo>}</m:mo></m:mrow></m:math> be the scale of functional classes <m:math><m:msub><m:mi mathvariant="script">F</m:mi><m:mi>α</m:mi></m:msub></m:math> indexed by <m:math><m:mrow><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi><m:mo>.</m:mo></m:mrow></m:math> Denote <m:math><m:mrow><m:msub><m:mi>R</m:mi><m:mi>n</m:mi></m:msub><m:mrow><m:mo>(</m:mo><m:mi>α</m:mi><m:mo>,</m:mo><m:mi>p</m:mi><m:mo>)</m:mo></m:mrow></m:mrow></m:math> the minimax risk over <m:math><m:msub><m:mi mathvariant="script">F</m:mi><m:mi>α</m:mi></m:msub></m:math> for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>loss:</para>
        <equation id="id2261373">
          <m:math mode="display">
            <m:mrow>
              <m:msub>
                <m:mi>R</m:mi>
                <m:mi>n</m:mi>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>α</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mo>=</m:mo>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">inf</m:mo>
                <m:msub>
                  <m:mover accent="true">
                    <m:mi>m</m:mi>
                    <m:mo>^</m:mo>
                  </m:mover>
                  <m:mi>n</m:mi>
                </m:msub>
              </m:munder>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:msub>
                    <m:mi mathvariant="script">F</m:mi>
                    <m:mi>α</m:mi>
                  </m:msub>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msub>
                    <m:mover accent="true">
                      <m:mi>m</m:mi>
                      <m:mo>^</m:mo>
                    </m:mover>
                    <m:mi>n</m:mi>
                  </m:msub>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <note id="id2261493" type="Definition"><label>Definition</label> 
The estimator <m:math><m:msubsup><m:mi>m</m:mi><m:mi>n</m:mi><m:mo>*</m:mo></m:msubsup></m:math> is called <emphasis>rate adaptive</emphasis> for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>loss and the scale of classes <m:math><m:mrow><m:msub><m:mi mathvariant="script">F</m:mi><m:mi>α</m:mi></m:msub><m:mo>,</m:mo><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi></m:mrow></m:math>  if for any <m:math><m:mrow><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi></m:mrow></m:math>
there exists <m:math><m:mrow><m:msub><m:mi>c</m:mi><m:mi>α</m:mi></m:msub><m:mo>&gt;</m:mo><m:mn>0</m:mn></m:mrow></m:math> such that</note>
        <equation id="id2261605">
          <m:math mode="display">
            <m:mrow>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:msub>
                    <m:mi mathvariant="script">F</m:mi>
                    <m:mi>α</m:mi>
                  </m:msub>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msubsup>
                    <m:mi>m</m:mi>
                    <m:mi>n</m:mi>
                    <m:mo>*</m:mo>
                  </m:msubsup>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>≤</m:mo>
              <m:msub>
                <m:mi>c</m:mi>
                <m:mi>α</m:mi>
              </m:msub>
              <m:msub>
                <m:mi>R</m:mi>
                <m:mi>n</m:mi>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>α</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mspace width="0.222222em"/>
              <m:mo>∀</m:mo>
              <m:mi>n</m:mi>
              <m:mo>≥</m:mo>
              <m:mn>1</m:mn>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2261718">The estimator <m:math><m:msubsup><m:mi>m</m:mi><m:mi>n</m:mi><m:mo>*</m:mo></m:msubsup></m:math> is called <emphasis>adaptive up to a logarithmic factor</emphasis> for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mi>p</m:mi></m:msub><m:mo>-</m:mo></m:mrow></m:math>loss and the scale of classes <m:math><m:mrow><m:msub><m:mi mathvariant="script">F</m:mi><m:mi>α</m:mi></m:msub><m:mo>,</m:mo><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi></m:mrow></m:math> if for any <m:math><m:mrow><m:mi>α</m:mi><m:mo>∈</m:mo><m:mi>A</m:mi></m:mrow></m:math> there exist <m:math><m:mrow><m:msub><m:mi>c</m:mi><m:mi>α</m:mi></m:msub><m:mo>&gt;</m:mo><m:mn>0</m:mn></m:mrow></m:math> and <m:math><m:mrow><m:mi>γ</m:mi><m:mo>=</m:mo><m:msub><m:mi>γ</m:mi><m:mi>α</m:mi></m:msub><m:mo>&gt;</m:mo><m:mn>0</m:mn></m:mrow></m:math> such that</para>
        <equation id="id2261853">
          <m:math mode="display">
            <m:mrow>
              <m:munder>
                <m:mo movablelimits="true" form="prefix">sup</m:mo>
                <m:mrow>
                  <m:mi>m</m:mi>
                  <m:mo>∈</m:mo>
                  <m:msub>
                    <m:mi mathvariant="script">F</m:mi>
                    <m:mi>α</m:mi>
                  </m:msub>
                </m:mrow>
              </m:munder>
              <m:mi>E</m:mi>
              <m:msubsup>
                <m:mfenced separators="" open="∥" close="∥">
                  <m:msubsup>
                    <m:mi>m</m:mi>
                    <m:mi>n</m:mi>
                    <m:mo>*</m:mo>
                  </m:msubsup>
                  <m:mo>-</m:mo>
                  <m:mi>m</m:mi>
                </m:mfenced>
                <m:mi>p</m:mi>
                <m:mi>p</m:mi>
              </m:msubsup>
              <m:mo>≤</m:mo>
              <m:msub>
                <m:mi>c</m:mi>
                <m:mi>α</m:mi>
              </m:msub>
              <m:msup>
                <m:mrow>
                  <m:mo>(</m:mo>
                  <m:mo form="prefix">log</m:mo>
                  <m:mi>n</m:mi>
                  <m:mo>)</m:mo>
                </m:mrow>
                <m:mi>γ</m:mi>
              </m:msup>
              <m:msub>
                <m:mi>R</m:mi>
                <m:mi>n</m:mi>
              </m:msub>
              <m:mrow>
                <m:mo>(</m:mo>
                <m:mi>α</m:mi>
                <m:mo>,</m:mo>
                <m:mi>p</m:mi>
                <m:mo>)</m:mo>
              </m:mrow>
              <m:mspace width="0.222222em"/>
              <m:mo>∀</m:mo>
              <m:mi>n</m:mi>
              <m:mo>≥</m:mo>
              <m:mn>1</m:mn>
              <m:mo>.</m:mo>
            </m:mrow>
          </m:math>
        </equation>
        <para id="id2261983">Thus, adaptive estimators have an optimal rate of convergence and behave as if they know in advance in which class the function to be estimated lies.</para>
        <para id="id2261989">The VisuShrink  procedure is adaptive up to a logarithmic factor for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>loss over every Besov, Hölder and Sobolev class that is contained in <m:math><m:mrow><m:mi>C</m:mi><m:mo>[</m:mo><m:mn>0</m:mn><m:mo>,</m:mo><m:mn>1</m:mn><m:mo>]</m:mo></m:mrow></m:math>, see Theorem 1.2 in <link target-id="bid7"/>.
The SureShrink  estimator does better: it is adaptive for the <m:math><m:mrow><m:msub><m:mi>L</m:mi><m:mn>2</m:mn></m:msub><m:mo>-</m:mo></m:mrow></m:math>loss, for a large scale of Besov, Hölder and Sobolev classes, see Theorem 1 in <link target-id="bid11"/>.</para>
     
    </section><section id="element-752"><title>Conclusion</title>
      
      <para id="id2253739">In this chapter, we saw the basic properties of standard wavelet theory and explained how these are related to the construction of wavelet regression estimators.</para>
    </section>
  </content>
  <bib:file>
    <bib:entry id="bid19">
      <bib:incollection>
<!--required fields-->
        <bib:author>Abramovich, F. and Benjamini, B.Y.</bib:author>
        <bib:title>Thresholding of wavelet coefficients as multiple hypotheses testing procedure</bib:title>
        <bib:booktitle>Wavelets in Statistics, Lectures Notes in Statistics, Vol. 103.</bib:booktitle>
        <bib:publisher>Springer-Verlag</bib:publisher>
        <bib:year>1995</bib:year>
<!--optional fields-->
        <bib:editor>Antoniadis, A. and Oppenheim, G.</bib:editor>
        <bib:number/>
        <bib:series/>
        <bib:type/>
        <bib:chapter/>
        <bib:pages>5-14</bib:pages>
        <bib:address/>
        <bib:edition/>
        <bib:month/>
        <bib:note/>
      </bib:incollection>
    </bib:entry>
    <bib:entry id="bid4">
      <bib:article>
<!--required fields-->
        <bib:author>Antoniadis, A. and Fan, J.</bib:author>
        <bib:title>Regularization of Wavelets approximations (with discussion)</bib:title>
        <bib:journal>J. Am. Statist. Assoc.</bib:journal>
        <bib:year>2001</bib:year>
<!--optional fields-->
        <bib:volume>96</bib:volume>
        <bib:number/>
        <bib:pages>939-967</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid1">
      <bib:article>
<!--required fields-->
        <bib:author>Antoniadis, A. and Grégoire, G. and McKeague, I.</bib:author>
        <bib:title>Wavelet methods for curve estimation.</bib:title>
        <bib:journal>J. Am. Statist. Assoc.</bib:journal>
        <bib:year>1994</bib:year>
<!--optional fields-->
        <bib:volume>89</bib:volume>
        <bib:number/>
        <bib:pages>1340-1353</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid16">
      <bib:techreport>
<!--required fields-->
        <bib:author>Antoniadis, A. and Leporini, D. and Pesquet, J.C.</bib:author>
        <bib:title>Wavelet thresholding for some classes of non-Gaussian noise</bib:title>
        <bib:institution>IMAG, Grenoble, France</bib:institution>
        <bib:year>2000</bib:year>
<!--optional fields-->
        <bib:type>Technical report</bib:type>
        <bib:number/>
        <bib:address/>
        <bib:month/>
        <bib:note/>
      </bib:techreport>
    </bib:entry>
    <bib:entry id="bid15">
      <bib:article>
<!--required fields-->
        <bib:author>Abramovich, F. and Sapatinas, T. and Silverman, B.W.</bib:author>
        <bib:title>Wavelet Thresholding via a Bayesian Approach</bib:title>
        <bib:journal>J. Roy. Statist. Soc., Series B</bib:journal>
        <bib:year>1998</bib:year>
<!--optional fields-->
        <bib:volume>60</bib:volume>
        <bib:number/>
        <bib:pages>725-749</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid5">
      <bib:article>
<!--required fields-->
        <bib:author>Bruce, A.G. and Gao, H.-Y.</bib:author>
        <bib:title>Waveshrink with firm shrinkage</bib:title>
        <bib:journal>Statistica Sinica</bib:journal>
        <bib:year>1997</bib:year>
<!--optional fields-->
        <bib:volume>4</bib:volume>
        <bib:number/>
        <bib:pages>855-874</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid9">
      <bib:techreport>
<!--required fields-->
        <bib:author>Berkner, K. and Wells, R.O.</bib:author>
        <bib:title>A correlation-dependent model for denoising via nonorthogonal wavelet transforms</bib:title>
        <bib:institution>Computational Mathematics Laboratory, Rice University</bib:institution>
        <bib:year>1998</bib:year>
<!--optional fields-->
        <bib:type>Technical report</bib:type>
        <bib:number/>
        <bib:address/>
        <bib:month/>
        <bib:note/>
      </bib:techreport>
    </bib:entry>
    <bib:entry id="bid13">
      <bib:article>
<!--required fields-->
        <bib:author>Cai, T.</bib:author>
        <bib:title>Adaptive wavelet estimation: a block thresholding and oracle inequality approach.</bib:title>
        <bib:journal>Annals of Statistics</bib:journal>
        <bib:year>1999</bib:year>
<!--optional fields-->
        <bib:volume>27</bib:volume>
        <bib:number/>
        <bib:pages>898-924</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid14">
      <bib:article>
<!--required fields-->
        <bib:author>Cai, T. and Silverman, B.W.</bib:author>
        <bib:title>Incorporating information on neighboring coefficients into wavelet estimation.</bib:title>
        <bib:journal>Sankhya: The Indian Journal of Statistics. Special Issue on Wavelets</bib:journal>
        <bib:year>2001</bib:year>
<!--optional fields-->
        <bib:volume>63</bib:volume>
        <bib:number/>
        <bib:pages>127-148</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid10">
      <bib:article>
<!--required fields-->
        <bib:author>Donoho, D.L. and Johnstone, I.M.</bib:author>
        <bib:title>Ideal spatial adaptation by wavelet shrinkage</bib:title>
        <bib:journal>Biometrika</bib:journal>
        <bib:year>1994</bib:year>
<!--optional fields-->
        <bib:volume>81</bib:volume>
        <bib:number/>
        <bib:pages>425-455</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid11">
      <bib:article>
<!--required fields-->
        <bib:author>Donoho, D.L. and Johnstone, I.M.</bib:author>
        <bib:title>Adapting to unknown smoothness via wavelet shrinkage.</bib:title>
        <bib:journal>J. Am. Statist. Assoc.</bib:journal>
        <bib:year>1995</bib:year>
<!--optional fields-->
        <bib:volume>90</bib:volume>
        <bib:number/>
        <bib:pages>1200-1224</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid7">
      <bib:article>
<!--required fields-->
        <bib:author>Donoho, D.L.</bib:author>
        <bib:title>De-noising via soft-thresholding</bib:title>
        <bib:journal>IEEE Transactions on Information Theory</bib:journal>
        <bib:year>1995</bib:year>
<!--optional fields-->
        <bib:volume>41</bib:volume>
        <bib:number/>
        <bib:pages>613-627</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid6">
      <bib:article>
<!--required fields-->
        <bib:author>Gao, H.-Y.</bib:author>
        <bib:title>Wavelet shrinkage denoising using the non-negative garrote</bib:title>
        <bib:journal>Journal of Computational and Graphical Statistics</bib:journal>
        <bib:year>1998</bib:year>
<!--optional fields-->
        <bib:volume>7</bib:volume>
        <bib:number/>
        <bib:pages>469-488</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid0">
      <bib:book>
<!--required fields-->
        <bib:author>Härdle, W. and Kerkyacharian, G. and Picard, D. and Tsybakov, A.</bib:author>
        <bib:title>Wavelets, Approximation, and Statistical Applications</bib:title>
        <bib:publisher>Springer-Verlag</bib:publisher>
        <bib:year>1998</bib:year>
<!--optional fields-->
        <bib:volume/>
        <bib:series>Lecture Notes in Statistics 129</bib:series>
        <bib:address/>
        <bib:edition/>
        <bib:month/>
        <bib:note/>
      </bib:book>
    </bib:entry>
    <bib:entry id="bid23">
      <bib:book>
<!--required fields-->
        <bib:author>Ibragimov, I.A. and Hasminskii, R.Z.</bib:author>
        <bib:title>Statistical Estimation: Asymptotic Theory</bib:title>
        <bib:publisher>Springer-Verlag</bib:publisher>
        <bib:year>1981</bib:year>
<!--optional fields-->
        <bib:volume/>
        <bib:series/>
        <bib:address>New York</bib:address>
        <bib:edition/>
        <bib:month/>
        <bib:note/>
      </bib:book>
    </bib:entry>
    <bib:entry id="bid21">
      <bib:article>
<!--required fields-->
        <bib:author>Jansen, M. and Bultheel, A.</bib:author>
        <bib:title>Multiple wavelet threshold estimation by generalized cross validation for images with correlated noise</bib:title>
        <bib:journal>IEEE Transactions on Image Processing</bib:journal>
        <bib:year>1999</bib:year>
<!--optional fields-->
        <bib:volume>8</bib:volume>
        <bib:number>7</bib:number>
        <bib:pages>947-953</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid22">
      <bib:article>
<!--required fields-->
        <bib:author>Jansen, M. and Malfait, M. and Bultheel, A.</bib:author>
        <bib:title>Generalized Cross Validation for wavelet thresholding</bib:title>
        <bib:journal>Signal Processing</bib:journal>
        <bib:year>1997</bib:year>
<!--optional fields-->
        <bib:volume>56</bib:volume>
        <bib:number/>
        <bib:pages>33-44</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid17">
      <bib:unpublished>
<!--required fields-->
        <bib:author>Johnstone, I.M. and Silverman., B.W.</bib:author>
        <bib:title>Empirical Bayes selection of wavelet thresholds</bib:title>
        <bib:note>Stanford University and University of Bristol</bib:note>
<!--optional fields-->
        <bib:month/>
        <bib:year>2002</bib:year>
      </bib:unpublished>
    </bib:entry>
    <bib:entry id="bid18">
      <bib:unpublished>
<!--required fields-->
        <bib:author>Johnstone, I.M. and Silverman., B.W.</bib:author>
        <bib:title>Finding needles and hay in haystacks: Risk bounds for Empirical Bayes estimates of possibly sparse sequences</bib:title>
        <bib:note>Stanford University and University of Bristol</bib:note>
<!--optional fields-->
        <bib:month/>
        <bib:year>2002</bib:year>
      </bib:unpublished>
    </bib:entry>
    <bib:entry id="bid8">
      <bib:article>
<!--required fields-->
        <bib:author>Johnstone, I.M. and Silverman, B.W.</bib:author>
        <bib:title>Wavelet methods for data with correlated noise</bib:title>
        <bib:journal>J. Roy. Statist. Soc., Series B</bib:journal>
        <bib:year>1997</bib:year>
<!--optional fields-->
        <bib:volume>59</bib:volume>
        <bib:number/>
        <bib:pages>319-351</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid20">
      <bib:techreport>
<!--required fields-->
        <bib:author>Nason, G.</bib:author>
        <bib:title>Wavelet regression by cross-validation</bib:title>
        <bib:institution>Stanford University</bib:institution>
        <bib:year>1994</bib:year>
<!--optional fields-->
        <bib:type>Technical report</bib:type>
        <bib:number/>
        <bib:address/>
        <bib:month/>
        <bib:note/>
      </bib:techreport>
    </bib:entry>
    <bib:entry id="bid2">
      <bib:techreport>
<!--required fields-->
        <bib:author>Nason, G.P.</bib:author>
        <bib:title>Fast cross-validatory choice of wavelet smoothness, primary resolution and threshold in wavelet shrinkage using the Kovac-Silverman algorithm.</bib:title>
        <bib:institution>University of Bristol</bib:institution>
        <bib:year>1999</bib:year>
<!--optional fields-->
        <bib:type>Technical report</bib:type>
        <bib:number/>
        <bib:address/>
        <bib:month/>
        <bib:note/>
      </bib:techreport>
    </bib:entry>
    <bib:entry id="bid3">
      <bib:article>
<!--required fields-->
        <bib:author>Sweldens, W. and Piessens, R.</bib:author>
        <bib:title>Quadrature Formulae and Asymptotic Error Expansions for wavelet approximations of smooth functions</bib:title>
        <bib:journal>SIAM J. Numer. Anal.</bib:journal>
        <bib:year>1994</bib:year>
<!--optional fields-->
        <bib:volume>31</bib:volume>
        <bib:number>4</bib:number>
        <bib:pages>1240-1264</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid12">
      <bib:article>
<!--required fields-->
        <bib:author>Stein, C.M.</bib:author>
        <bib:title>Estimation of the mean of a multivariate normal distribution</bib:title>
        <bib:journal>Annals of Statistics</bib:journal>
        <bib:year>1981</bib:year>
<!--optional fields-->
        <bib:volume>9</bib:volume>
        <bib:number/>
        <bib:pages>1135-1151</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
    <bib:entry id="bid24">
      <bib:article>
<!--required fields-->
        <bib:author>Stone, C.J.</bib:author>
        <bib:title>Optimal global rates of convergence for nonparametric regression.</bib:title>
        <bib:journal>Annals of Statistics</bib:journal>
        <bib:year>1982</bib:year>
<!--optional fields-->
        <bib:volume>10</bib:volume>
        <bib:number/>
        <bib:pages>1040-1053</bib:pages>
        <bib:month/>
        <bib:note/>
      </bib:article>
    </bib:entry>
  </bib:file>
</document>